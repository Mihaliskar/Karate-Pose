import os
import sys
import argparse
import torch
import numpy as np
import cv2
from loguru import logger
from glob import glob
from train.core.tester import Tester
from train.utils.one_euro_filter import OneEuroFilter
from multi_person_tracker import MPT
from multi_person_tracker import Sort
#Dataloader
from torch.utils.data import DataLoader
#from train.core.tester_smpl import Tester
os.environ['PYOPENGL_PLATFORM'] = 'egl'
#os.environ["DISPLAY"] = ":0"e
sys.path.append('')


"""
Check if a path exists
"""
def checkIfPathIsDirectory(filename):
    if (filename is None):  
       return False
    return os.path.isdir(filename) 

"""
Easy way to switch inputs
"""
def getCaptureDeviceFromPath(videoFilePath,videoWidth,videoHeight,videoFramerate=30):
  #------------------------------------------
  if (videoFilePath=="esp"):
     from espStream import ESP32CamStreamer
     cap = ESP32CamStreamer()
  if (videoFilePath=="screen"):
     from screenStream import ScreenGrabber
     cap =  ScreenGrabber(region=(0,0,videoWidth,videoHeight))
  elif (videoFilePath=="webcam"):
     cap = cv2.VideoCapture(0)
     cap.set(cv2.CAP_PROP_FPS,videoFramerate)
     cap.set(cv2.CAP_PROP_FRAME_WIDTH, videoWidth)
     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, videoHeight)
  elif (videoFilePath=="/dev/video0"):
     cap = cv2.VideoCapture(0)
     cap.set(cv2.CAP_PROP_FPS,videoFramerate)
     cap.set(cv2.CAP_PROP_FRAME_WIDTH, videoWidth)
     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, videoHeight)
  elif (videoFilePath=="/dev/video1"):
     cap = cv2.VideoCapture(1)
     cap.set(cv2.CAP_PROP_FPS,videoFramerate)
     cap.set(cv2.CAP_PROP_FRAME_WIDTH, videoWidth)
     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, videoHeight)
  elif (videoFilePath=="/dev/video2"):
     cap = cv2.VideoCapture(2)
     cap.set(cv2.CAP_PROP_FPS,videoFramerate)
     cap.set(cv2.CAP_PROP_FRAME_WIDTH, videoWidth)
     cap.set(cv2.CAP_PROP_FRAME_HEIGHT, videoHeight)
  else:
     if (checkIfPathIsDirectory(videoFilePath) and (not "/dev/" in videoFilePath) ):
        from folderStream import FolderStreamer
        cap = FolderStreamer(path=videoFilePath,width=videoWidth,height=videoHeight)
     else:
        cap = cv2.VideoCapture(videoFilePath)
  return cap 

"""
Select joints that we want from SMPL skeleton
"""
def get_smpl_skeleton():
     return np.array(
                                [
                                    [ 0, 1 ],
                                    [ 0, 2 ],
                                    [ 0, 3 ],
                                    [ 1, 4 ],
                                    [ 2, 5 ],
                                    [ 3, 6 ],
                                    [ 4, 7 ],
                                    [ 5, 8 ],
                                    [ 6, 9 ],
                                    [ 7, 10],
                                    [ 8, 11],
                                    [ 9, 12],
                                    [ 9, 13],
                                    [ 9, 14],
                                    [12, 15],
                                    [13, 16],
                                    [14, 17],
                                    [16, 18],
                                    [17, 19],
                                    [18, 20],
                                    [19, 21],

                                ]
                            )



def get_colors():
     colors = {
                                'pink': np.array([197, 27, 125]),  # L lower leg
                                'light_pink': np.array([233, 163, 201]),  # L upper leg
                                'light_green': np.array([161, 215, 106]),  # L lower arm
                                'green': np.array([77, 146, 33]),  # L upper arm
                                'red': np.array([215, 48, 39]),  # head
                                'light_red': np.array([252, 146, 114]),  # head
                                'light_orange': np.array([252, 141, 89]),  # chest
                                'purple': np.array([118, 42, 131]),  # R lower leg
                                'light_purple': np.array([175, 141, 195]),  # R upper
                                'light_blue': np.array([145, 191, 219]),  # R lower arm
                                'blue': np.array([69, 117, 180]),  # R upper arm
                                'gray': np.array([130, 130, 130]),  #
                                'white': np.array([255, 255, 255]),  #
                                'pinkish': np.array([204, 77, 77]),
                            }
     return colors




def save_matlab_visualization(hmr_output,output_filename="skeleton.png"):
    from matplotlib import pyplot as plt
                       
    joints     = hmr_output['joints3d'].cpu().numpy()
    hmr_joints = hmr_output['joints3d'][:, 0:22, :].cpu().numpy()
    camera_translation = hmr_output['pred_cam_t'].cpu().numpy() * 0.5 
    ax = None

    radius =1
    skeleton = get_smpl_skeleton()
    #joints = joints[:len(skeleton)+1]
    if True:
                            fig = plt.figure(figsize=(12, 7))
                            ax = fig.add_subplot(111, projection='3d')
                            ax.set_aspect('auto')
    kp_3d = joints[0]
    for i, (j1, j2) in enumerate(skeleton):
                                if kp_3d[j1].shape[0] == 4:
                                    x, y, z, v = [np.array([kp_3d[j1, c], kp_3d[j2, c]]) for c in range(4)]
                                else:
                                    x, y, z = [np.array([kp_3d[j1, c], kp_3d[j2, c]]) for c in range(3)]
                                    v = [1, 1]
                                ax.plot(x, y, z, lw=2, c=get_colors()['purple'] / 255)
                                for j in range(2):
                                    if v[j] > 0: # if visible
                                        ax.plot(x[j], y[j], z[j], lw=2, c=get_colors()['blue'] / 255, marker='o')
                                    else: # nonvisible
                                        ax.plot(x[j], y[j], z[j], lw=2, c=get_colors()['red'] / 255, marker='x')

    pelvis_joint = 0
    RADIUS = radius  # space around the subject
    xroot, yroot, zroot = kp_3d[pelvis_joint, 0], kp_3d[pelvis_joint, 1], kp_3d[pelvis_joint, 2]
    ax.set_xlim3d([-RADIUS + xroot, RADIUS + xroot])
    ax.set_zlim3d([-RADIUS + zroot, RADIUS + zroot])
    ax.set_ylim3d([-RADIUS + yroot, RADIUS + yroot])

    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_zlabel("z")
    ax.view_init(-90, -90)
    if ax is None:
                            plt.show()
    plt.savefig(output_filename)


def saveCSVFileFromListOfDicts(filename,inputDicts):
    labels = list()
    #--------------------------
    for frame in inputDicts:
      for label in frame.keys():
        if not label in labels: 
           labels.append(label)
    #--------------------------   
    f = open(filename, 'w')
    #Write header..
    #------------------------------------------------------------------------  
    for column in range(len(labels)):
      if (column>0):
        f.write(',')
      f.write("%s_3DX,"%(labels[column]))
      f.write("%s_3DY,"%(labels[column]))
      f.write("%s_3DZ" %(labels[column]))
    f.write('\n')
    #------------------------------------------------------------------------  
    #Write body..
    #------------------------------------------------------------------------ 
    for frame in inputDicts: 
     for column in range(len(labels)):
      if (column>0):
        f.write(',')
      if (labels[column] in frame):
         f.write("%f,"%(frame[labels[column]][0]))
         f.write("%f,"%(frame[labels[column]][1]))
         f.write("%f" %(frame[labels[column]][2]))
      else:
         f.write("0")
     f.write('\n')
    #------------------------------------------------------------------------  
    f.close()


def encode_smpl_skeleton_to_dict(hmr_output): 
    # Extract the first sample in the batch and take the first 22 SMPL joints
    joints_3d = hmr_output['joints3d'][0, :22, :]

    # Convert to numpy if needed
    if isinstance(joints_3d, torch.Tensor):
        joints_3d = joints_3d.cpu().numpy()

    SMPL_JOINT_NAMES = [
    "pelvis", "left_hip", "right_hip", "spine1", "left_knee", "right_knee",
    "spine2", "left_ankle", "right_ankle", "spine3", "left_foot", "right_foot",
    "neck", "left_collar", "right_collar", "head", "left_shoulder",
    "right_shoulder", "left_elbow", "right_elbow", "left_wrist", "right_wrist"]

    # Encode as a dictionary
    joints_dict = {SMPL_JOINT_NAMES[i]: joints_3d[i].tolist() for i in range(joints_3d.shape[0])}

    return joints_dict


def encode_smplx_skeleton_to_dict(hmr_output): 
    # --- Extract body joints ---
    joints_body = hmr_output['joints3d'][0]  # shape (22, 3)

    if isinstance(joints_body, torch.Tensor):
        joints_body = joints_body.cpu().numpy()

    SMPL_JOINT_NAMES = [
        "pelvis", "left_hip", "right_hip", "spine1", "left_knee", "right_knee",
        "spine2", "left_ankle", "right_ankle", "spine3", "left_foot", "right_foot",
        "neck", "left_collar", "right_collar", "head", "left_shoulder",
        "right_shoulder", "left_elbow", "right_elbow", "left_wrist", "right_wrist"
    ]

    joints_dict = {SMPL_JOINT_NAMES[i]: joints_body[i].tolist() for i in range(joints_body.shape[0])}

    # --- Hand joint names based on MANO/SMPL-X ---
    HAND_JOINT_NAMES = [
        'wrist',
        'thumb1', 'thumb2', 'thumb3', 'thumb4',
        'index1', 'index2', 'index3', 'index4',
        'middle1', 'middle2', 'middle3', 'middle4',
        'ring1', 'ring2', 'ring3', 'ring4',
        'pinky1', 'pinky2', 'pinky3', 'pinky4'
    ]

    # --- Add hands ---
    for side in ['left', 'right']:
        hand_key_3d = f"{side}_hand_3d"
        if hand_key_3d in hmr_output:
            hand_joints = hmr_output[hand_key_3d][0]  # shape (15, 3) or 21?
            if isinstance(hand_joints, torch.Tensor):
                hand_joints = hand_joints.cpu().numpy()
            
            # If hand has fewer joints than HAND_JOINT_NAMES, truncate names
            names_to_use = HAND_JOINT_NAMES[:hand_joints.shape[0]]
            for name, joint in zip(names_to_use, hand_joints):
                joints_dict[f"{side}_hand_{name}"] = joint.tolist()

    # --- Head joint names (facial landmarks) ---
    HEAD_JOINT_NAMES = [
        'jaw', 'mouth_left', 'mouth_right', 'mouth_top', 'mouth_bottom',
        'nose', 'left_eye', 'right_eye', 'left_eyebrow', 'right_eyebrow'
    ]
    # If more joints are available, just number them
    if 'head_3d' in hmr_output:
        head_joints = hmr_output['head_3d'][0]
        if isinstance(head_joints, torch.Tensor):
            head_joints = head_joints.cpu().numpy()
        
        for i, joint in enumerate(head_joints):
            if i < len(HEAD_JOINT_NAMES):
                joints_dict[f"head_{HEAD_JOINT_NAMES[i]}"] = joint.tolist()
            else:
                joints_dict[f"head_extra_{i}"] = joint.tolist()

    return joints_dict



def save_skeleton_dict_to_json(skeleton_dict, output_filename="skeleton.json"):
    import json
    # Ensure the output directory exists
    directoryPath = os.path.dirname(output_filename)
  
    if (directoryPath!=""):
       os.makedirs(directoryPath, exist_ok=True)

    # Save as pretty-printed JSON
    with open(output_filename, "w") as f:
        json.dump(skeleton_dict, f, indent=4)

    print(f"[INFO] Skeleton saved to {output_filename}")


def smpl_to_32joints(hmr_output):
    """
    Converts SMPL (22x3) joints from hmr_output to a 32-joint skeleton format.

    Args:
        hmr_output (dict): Contains 'joints3d' tensor of shape (B, 22, 3)

    Returns:
        np.ndarray: Array of shape (32, 3)
    """
    # Extract 22 joints
    joints_22 = hmr_output['joints3d'][0, :22, :]
    if isinstance(joints_22, torch.Tensor):
        joints_22 = joints_22.cpu().numpy()

    # Initialize 32-joint array
    joints_32 = np.zeros((32, 3))

    # Basic mapping (fill directly from SMPL)
    mapping = {
        0: 0,  1: 3,  2: 6,  3: 9,  4: 12,
        5: 13, 6: 14, 7: 16, 8: 18, 9: 20,
        10: 17, 11: 19, 12: 21,
        15: 1, 16: 2, 17: 4, 18: 5,
        19: 7, 20: 8, 21: 10, 22: 11,
        23: 15
    }

    for target, source in mapping.items():
        joints_32[target] = joints_22[source]

    # Estimate head top (24) by extending the neck-head vector
    neck = joints_32[4]
    head = joints_32[23]
    joints_32[24] = head + 0.25 * (head - neck)

    # Duplicate hands (13, 14)
    joints_32[13] = joints_32[9]
    joints_32[14] = joints_32[12]

    # Fill remaining joints (25â€“31) as small variations or copies of nearby points
    for i in range(25, 32):
        joints_32[i] = joints_32[24]  # could be refined if you have dataset definitions

    return joints_32


def main(args):

    input_image_folder = args.image_folder
    output_path        = args.output_folder
    #os.makedirs(output_path, exist_ok=True)

    logger.add(
        os.path.join(output_path, 'demo.log'),
        level='INFO',
        colorize=False,
    )
    logger.info(f'Demo options: \n {args}')
    bbox_one_euro_filter = OneEuroFilter(
        np.zeros(4),
        np.zeros(4),
        min_cutoff=0.004,
        beta=0.4,
    )

    tester  = Tester(args)
    history = list()

    if True:
        all_image_folder = [input_image_folder]
        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        torch.set_float32_matmul_precision('medium')
        with torch.cuda.amp.autocast(), torch.no_grad():
            mot = MPT(
                device=torch.device('cuda'),
                batch_size=4,
                display=False,
                detector_type='yolo',
                output_format='dict',
                yolo_img_size=416
            )
            videoWidth     = 1280
            videoHeight    = 720
            videoFramerate = 30 
            cap = getCaptureDeviceFromPath(args.input,videoWidth,videoHeight,videoFramerate)

            frameNumber = 0
            use_bbox_filter = False
            while True:
                frameNumber+=1
                if True:#frameNumber%2==0:
                    ret, frame = cap.read()          
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    input_tensor = torch.tensor(frame).permute(2, 0, 1).unsqueeze(0) / 255.0
                    detection    = mot.detector(input_tensor.cuda())

                    # Concatenate boxes and scores from all predictions at once
                    if detection:
                        #import ipdb; ipdb.set_trace()
                        t = torch.ones(4) * frameNumber
                        if use_bbox_filter:
                            boxes = torch.cat([bbox_one_euro_filter(t,pred['boxes']) for pred in detection], dim=0)
                        else:
                            boxes = torch.cat([pred['boxes'] for pred in detection], dim=0)
                        scores = torch.cat([pred['scores'] for pred in detection], dim=0)
                        # Apply threshold in a vectorized way
                        mask = scores > 0.7
                        # Filter and add scores as a new column
                        filtered_boxes = boxes[mask]
                        filtered_scores = scores[mask].unsqueeze(1)
                        # Merge boxes and scores using concatenation
                        dets = torch.cat([filtered_boxes, filtered_scores], dim=1).cpu().detach().numpy()
                    else:
                        dets = np.empty((0, 5))
                        #print("No detection")

                    detections = [dets]
                    detection = mot.prepare_output_detections(detections)
                    if len(detection[0]) > 0:

                        saveFilename = None
                        if (args.save):
                           saveFilename = 'colorFrame_0_%05d.jpg' % frameNumber

                        hmr_output=tester.run_on_single_image_tensor(frame, detection, save=saveFilename)
                        #---------------------------------------------------------------------------------------
                        #At this point hmr_output has the resolved pose data..!
                        #---------------------------------------------------------------------------------------
                        
                        #pose3DAsDictionary = encode_smpl_skeleton_to_dict(hmr_output)
                        pose3DAsDictionary = encode_smplx_skeleton_to_dict(hmr_output)

                        history.append(pose3DAsDictionary)
 
                        #We can dump the skeleton to disk as skeleton_00000.json etc.
                        if (args.save):
                           save_skeleton_dict_to_json(pose3DAsDictionary,output_filename="skeleton_%05u.json" % frameNumber)

                        #Uncomment to also do a matlab visualization
                        #save_matlab_visualization(hmr_output,output_filename="skeleton_%05u.png" % frameNumber)
                    else:
                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                        cv2.imshow('front', frame)
                        if (args.save):
                           saveFilename = 'colorFrame_0_%05d.jpg' % frameNumber
                           cv2.imwrite(saveFilename, frame)
                        

                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

    saveCSVFileFromListOfDicts("3DPoints.csv",history)
    if (args.save):
        os.system("ffmpeg -framerate %u -start_number 1 -i colorFrame_0_%%05d.jpg -s %ux%u  -y -r %u -pix_fmt yuv420p -threads 8 livelastRun3DHiRes.mp4 && rm colorFrame_0_*.jpg " % (videoFramerate,videoWidth,videoHeight,videoFramerate)) # 
    del tester.model

    logger.info('================= END =================')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()


    parser.add_argument('--save', help='Save .json / visualization output', action=argparse.BooleanOptionalAction)

    parser.add_argument('--input', type=str, default='/dev/video0',
                        help='From Device (path to files, videos , /dev/videoX or screen )')

    parser.add_argument('--cfg', type=str, default='configs/dpose_conf.yaml',
                        help='config file that defines model hyperparams')

    parser.add_argument('--ckpt', type=str, default='data/ckpt/paper_arxiv.ckpt',
                        help='checkpoint path')

    parser.add_argument('--image_folder', type=str, default='demo_images',
                        help='input image folder')

    parser.add_argument('--output_folder', type=str, default='demo_images/results',
                        help='output folder to write results')

    parser.add_argument('--tracker_batch_size', type=int, default=1,
                        help='batch size of object detector used for bbox tracking')
                        
    parser.add_argument('--display', action='store_true',
                        help='visualize the 3d body projection on image')

    parser.add_argument('--detector', type=str, default='yolo', choices=['yolo', 'maskrcnn'],
                        help='object detector to be used for bbox tracking')

    parser.add_argument('--yolo_img_size', type=int, default=416,
                        help='input image size for yolo detector')
    parser.add_argument('--eval_dataset', type=str, default=None)
    parser.add_argument('--dataframe_path', type=str, default='data/ssp_3d_test.npz')
    parser.add_argument('--data_split', type=str, default='test')

    args = parser.parse_args()
    main(args)
